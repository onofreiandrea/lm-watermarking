{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "721c64a3",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5afb10c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\viene\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "from statistics import mean\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "import cmasher as cmr\n",
    "import sys\n",
    "from typing import List, Iterable, Optional\n",
    "from functools import partial\n",
    "import time\n",
    "import random\n",
    "import math\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from tokenizers import Tokenizer\n",
    "import torch_directml\n",
    "from datasets import load_dataset, Dataset\n",
    "from extended_watermark_processor import WatermarkLogitsProcessor, WatermarkDetector\n",
    "from transformers import (AutoTokenizer, \n",
    "                          AutoModelForSeq2SeqLM, \n",
    "                          AutoModelForCausalLM,\n",
    "                          LogitsProcessorList,\n",
    "                          BertTokenizer, \n",
    "                          BertForMaskedLM)\n",
    "\n",
    "rc('font', **{'family': 'serif', 'serif': ['Computer Modern']})\n",
    "rc('text', usetex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d6bfee",
   "metadata": {},
   "source": [
    "Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d92f33a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'instruction': 'Identify the conjunctions in the following sentence', 'input': 'She wrote a letter and sealed it', 'output': 'The conjunctions in the sentence are \"and\".', 'text': 'Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nIdentify the conjunctions in the following sentence\\n\\n### Input:\\nShe wrote a letter and sealed it\\n\\n### Response:\\nThe conjunctions in the sentence are \"and\".'}\n"
     ]
    }
   ],
   "source": [
    "dataset_name = \"tatsu-lab/alpaca\"\n",
    "\n",
    "dataset = load_dataset(dataset_name, split=\"train\", streaming=True, trust_remote_code=True)\n",
    "\n",
    "# log an example\n",
    "ds_iterator = iter(dataset)\n",
    "idx = 75 # if this is c4, it's the schumacher example lol\n",
    "i = 0\n",
    "while i < idx: \n",
    "    next(ds_iterator)\n",
    "    i += 1\n",
    "\n",
    "example = next(ds_iterator)\n",
    "print(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e0ef11",
   "metadata": {},
   "source": [
    "Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "715707d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OPTForCausalLM(\n",
       "  (model): OPTModel(\n",
       "    (decoder): OPTDecoder(\n",
       "      (embed_tokens): Embedding(50272, 2048, padding_idx=1)\n",
       "      (embed_positions): OPTLearnedPositionalEmbedding(2050, 2048)\n",
       "      (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x OPTDecoderLayer(\n",
       "          (self_attn): OPTSdpaAttention(\n",
       "            (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "            (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "            (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "            (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "          (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "          (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=50272, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_model_name = \"facebook/opt-1.3b\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(hf_model_name)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(hf_model_name)\n",
    "\n",
    "# defaults to device 0\n",
    "# will need to use 'parallelize' for multi-gpu sharding\n",
    "device = torch.device(\"cpu\")\n",
    "model = model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a079ca68",
   "metadata": {},
   "source": [
    "# T=1\n",
    "\n",
    "Generate output text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a242d0ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create a Twitter post to promote your new product.Alisha's All Natural Jam\n",
      "LLM response:\n",
      "\n",
      " is a jam of organic sugarcane juice, sugarcane molasses and sunflower seeds.\n",
      "\n",
      "Use a hashtag to promote your campaign. #allnaturaljam\n",
      "\n",
      "Write an email that you will forward to your fans. You should also include a link back to your website or social media page.\n",
      "\n",
      "You should be able to include a short video with your campaign.\n",
      "\n",
      "The more details you include on your social media page, the more likely your fans are to share your product on social media.\n",
      "\n",
      "Create a Facebook post with the hashtag #allnaturaljam\n",
      "\n",
      "Write an email about your campaign that your fans can follow and forward. You should also include a link back to your website or social media page.\n",
      "\n",
      "You should be able to include a short video with your campaign.\n",
      "\n",
      "The more details you include on your social media page, the more likely your fans are to share your product on social media.\n",
      "\n",
      "Create a short Google+ post with the\n"
     ]
    }
   ],
   "source": [
    "watermark_processor = WatermarkLogitsProcessor(vocab=list(tokenizer.get_vocab().values()),\n",
    "                                               gamma=0.25,\n",
    "                                               delta=2.0,\n",
    "                                               seeding_scheme=\"selfhash\") #equivalent to `ff-anchored_minhash_prf-4-True-15485863`\n",
    "# Note:\n",
    "# You can turn off self-hashing by setting the seeding scheme to `minhash`.\n",
    "\n",
    "example = next(ds_iterator)\n",
    "\n",
    "input = example[\"instruction\"] + example[\"input\"]\n",
    "\n",
    "tokenized_input = tokenizer(input, return_tensors='pt')\n",
    "\n",
    "tokenized_input = {k: v.to(device) for k, v in tokenized_input.items()}\n",
    "# note that if the model is on cuda, then the input is on cuda\n",
    "# and thus the watermarking rng is cuda-based.\n",
    "# This is a different generator than the cpu-based rng in pytorch!\n",
    "\n",
    "output_tokens = model.generate(**tokenized_input,\n",
    "                                do_sample=True, \n",
    "                                top_k=0,\n",
    "                                temperature=0.7,\n",
    "                                max_new_tokens=200,\n",
    "                                num_beams=1,\n",
    "                                logits_processor=LogitsProcessorList([watermark_processor])\n",
    "                                )\n",
    "\n",
    "# if decoder only model, then we need to isolate the\n",
    "# newly generated tokens as only those are watermarked, the input/prompt is not\n",
    "output_tokens = output_tokens[:,tokenized_input[\"input_ids\"].shape[-1]:]\n",
    "\n",
    "output_text = tokenizer.batch_decode(output_tokens, skip_special_tokens=True)[0]\n",
    "print(input)\n",
    "print(\"LLM response:\\n\")\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b718cab7",
   "metadata": {},
   "source": [
    "Evaluate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8558b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_tokens_scored': 132, 'num_green_tokens': 76, 'green_fraction': 0.5757575757575758, 'z_score': 8.643325211229223, 'p_value': 2.7300111309182428e-18, 'z_score_at_T': tensor([1.7321, 2.4495, 3.0000, 3.4641, 2.8402, 2.3570, 1.9640, 2.4495, 2.8868,\n",
      "        2.5560, 2.2630, 2.6667, 3.0424, 2.7775, 3.1305, 3.4641, 3.2206, 3.5382,\n",
      "        3.8411, 3.6148, 3.4017, 3.6927, 3.9727, 3.7712, 4.0415, 4.3027, 4.1111,\n",
      "        4.3644, 4.6101, 4.8488, 4.6663, 4.8990, 5.1257, 5.3468, 5.5626, 5.7735,\n",
      "        5.6000, 5.4322, 5.6395, 5.8424, 5.6805, 5.8797, 6.0751, 5.9186, 6.1107,\n",
      "        6.2993, 6.1477, 6.0000, 5.8560, 5.7155, 5.9017, 5.7646, 5.9479, 5.8140,\n",
      "        5.6830, 5.8635, 5.7354, 5.9132, 5.7877, 5.6647, 5.5442, 5.7192, 5.8919,\n",
      "        6.0622, 5.9438, 6.1118, 6.2776, 6.1612, 6.3249, 6.2106, 6.0982, 5.9876,\n",
      "        6.1492, 6.0404, 5.9333, 5.8279, 5.9874, 6.1450, 6.3008, 6.1968, 6.0943,\n",
      "        5.9932, 6.1471, 6.2994, 6.1996, 6.3502, 6.2517, 6.4008, 6.3035, 6.4510,\n",
      "        6.5970, 6.7416, 6.8849, 7.0268, 7.1674, 7.3068, 7.2104, 7.3485, 7.4853,\n",
      "        7.6210, 7.5258, 7.6603, 7.5661, 7.6995, 7.8318, 7.7387, 7.8699, 7.7778,\n",
      "        7.6867, 7.8168, 7.7268, 7.8558, 7.7667, 7.7667, 7.7667, 7.7667, 7.7667,\n",
      "        7.7667, 7.7667, 7.6785, 7.8065, 7.7192, 7.8463, 7.9724, 7.8859, 7.8003,\n",
      "        7.9254, 8.0497, 8.1731, 8.2956, 8.4173, 8.5381, 8.5381, 8.5381, 8.5381,\n",
      "        8.5381, 8.5381, 8.5381, 8.5381, 8.5381, 8.5381, 8.5381, 8.5381, 8.5381,\n",
      "        8.5381, 8.5381, 8.5381, 8.5381, 8.5381, 8.5381, 8.5381, 8.5381, 8.5381,\n",
      "        8.5381, 8.5381, 8.5381, 8.5381, 8.5381, 8.5381, 8.5381, 8.5381, 8.5381,\n",
      "        8.5381, 8.5381, 8.5381, 8.5381, 8.5381, 8.5381, 8.5381, 8.5381, 8.5381,\n",
      "        8.5381, 8.5381, 8.5381, 8.5381, 8.5381, 8.5381, 8.5381, 8.5381, 8.5381,\n",
      "        8.5381, 8.5381, 8.5381, 8.5381, 8.5381, 8.5381, 8.5381, 8.5381, 8.5381,\n",
      "        8.5381, 8.5381, 8.6581, 8.7773, 8.6924, 8.6083, 8.7267, 8.6433]), 'prediction': True, 'confidence': 1.0}\n"
     ]
    }
   ],
   "source": [
    "watermark_detector = WatermarkDetector(vocab=list(tokenizer.get_vocab().values()),\n",
    "                                        gamma=0.25, # should match original setting\n",
    "                                        seeding_scheme=\"selfhash\", # should match original setting\n",
    "                                        device=model.device, # must match the original rng device type\n",
    "                                        tokenizer=tokenizer,\n",
    "                                        z_threshold=4.0,\n",
    "                                        normalizers=[],\n",
    "                                        ignore_repeated_ngrams=True)\n",
    "\n",
    "score_dict = watermark_detector.detect(output_text) # or any other text of interest to analyze\n",
    "\n",
    "print(score_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dccf71e",
   "metadata": {},
   "source": [
    "Simultate paraphrased text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "90f1e6ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is a jam of organic sugarcane juice , sugarcane mochacha and sunflower oil . create a hashtag to promote your new product # allnaturaljam is an email that you send forward to your fans . you should also create a facebook back - up page or social media page . you should be able to include a short video with your campaign . the more details you include on your social media page , the more likely your fans are to share your product on social media . create a facebook post with the hashtag # allnaturalja and write some comments about your product that your fans can follow you forward . you should also include writing it back on your website or social media page . should also be able to share a short video with your campaign . the more details you include on your social media page , the more likely your fans are to share your product on social media . create a short , short video with the\n"
     ]
    }
   ],
   "source": [
    "model_name = \"bert-base-uncased\"\n",
    "paraphrase_tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "paraphrase_model = BertForMaskedLM.from_pretrained(model_name)\n",
    "paraphrase_model = paraphrase_model.to(device)\n",
    "\n",
    "def mask_tokens(tokens, mask_fraction=0.3):\n",
    "    \"\"\"Randomly mask some tokens\"\"\"\n",
    "    n_to_mask = max(1, int(len(tokens) * mask_fraction))\n",
    "    mask_indices = random.sample(range(1, len(tokens)-1), n_to_mask)  # skip [CLS] and [SEP]\n",
    "    masked_tokens = tokens.copy()\n",
    "    for idx in mask_indices:\n",
    "        masked_tokens[idx] = paraphrase_tokenizer.mask_token\n",
    "    return masked_tokens, mask_indices\n",
    "\n",
    "def iterative_paraphrase(text, mask_fraction=0.3):\n",
    "    # Tokenize\n",
    "    tokens = paraphrase_tokenizer.tokenize(text)\n",
    "    tokens = [paraphrase_tokenizer.cls_token] + tokens + [paraphrase_tokenizer.sep_token]\n",
    "    masked_tokens, mask_indices = mask_tokens(tokens, mask_fraction)\n",
    "    \n",
    "    for idx in mask_indices:\n",
    "        input_ids = paraphrase_tokenizer.convert_tokens_to_ids(masked_tokens)\n",
    "        input_tensor = torch.tensor([input_ids]).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = paraphrase_model(input_tensor)\n",
    "            predictions = outputs.logits\n",
    "        \n",
    "        topk = torch.topk(predictions[0, idx], k=50)\n",
    "        for token_id in topk.indices:\n",
    "            token = paraphrase_tokenizer.convert_ids_to_tokens([token_id.item()])[0]\n",
    "            if not token.startswith(\"[unused\") and token != paraphrase_tokenizer.unk_token:\n",
    "                predicted_token = token\n",
    "                break\n",
    "        else:\n",
    "            predicted_token = paraphrase_tokenizer.unk_token\n",
    "        \n",
    "        masked_tokens[idx] = predicted_token\n",
    "    \n",
    "    # Detokenize\n",
    "    return paraphrase_tokenizer.convert_tokens_to_string(masked_tokens[1:-1])  # remove [CLS], [SEP]\n",
    "\n",
    "paraphrased = iterative_paraphrase(output_text, mask_fraction=0.3)\n",
    "\n",
    "print(paraphrased)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51a98b7",
   "metadata": {},
   "source": [
    "Evaluate paraphrased text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8ecac6ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_tokens_scored': 147, 'num_green_tokens': 62, 'green_fraction': 0.4217687074829932, 'z_score': 4.809523809523809, 'p_value': 7.564513773304572e-07, 'z_score_at_T': tensor([1.7321, 2.4495, 3.0000, 3.4641, 2.8402, 2.3570, 1.9640, 2.4495, 2.8868,\n",
      "        2.5560, 2.2630, 2.6667, 2.4019, 2.7775, 2.5342, 2.3094, 2.1004, 1.9052,\n",
      "        1.7219, 2.0656, 2.3938, 2.7080, 2.5281, 2.8284, 2.6558, 2.4910, 2.7778,\n",
      "        3.0551, 3.3235, 3.1623, 3.4219, 3.2660, 3.1156, 2.9704, 2.8301, 2.6943,\n",
      "        2.9424, 2.8098, 3.0509, 3.2863, 3.1558, 3.3853, 3.6098, 3.4816, 3.3566,\n",
      "        3.2348, 3.1160, 3.0000, 2.8868, 2.7761, 2.9913, 2.8823, 2.7757, 2.6713,\n",
      "        2.8804, 2.7775, 2.6765, 2.8808, 3.0817, 2.9814, 3.1787, 3.3729, 3.2733,\n",
      "        3.4641, 3.3657, 3.2691, 3.1741, 3.3607, 3.2667, 3.1743, 3.0833, 2.9938,\n",
      "        2.9057, 2.8189, 2.7333, 2.9140, 3.0924, 3.0071, 3.1829, 3.0984, 3.0151,\n",
      "        2.9329, 3.1052, 3.2757, 3.4442, 3.6109, 3.7758, 3.9389, 4.1003, 4.0166,\n",
      "        4.1761, 4.3339, 4.4901, 4.4066, 4.3241, 4.4783, 4.3966, 4.3158, 4.2359,\n",
      "        4.1569, 4.3086, 4.2303, 4.3804, 4.3027, 4.2258, 4.1497, 4.2977, 4.2222,\n",
      "        4.1475, 4.0736, 4.2196, 4.1461, 4.2907, 4.4341, 4.5762, 4.5029, 4.6437,\n",
      "        4.5708, 4.7104, 4.6380, 4.5663, 4.7044, 4.7044, 4.6332, 4.5626, 4.4927,\n",
      "        4.6291, 4.5596, 4.6949, 4.6258, 4.5573, 4.6912, 4.6232, 4.6232, 4.6232,\n",
      "        4.7559, 4.6883, 4.8200, 4.7527, 4.8833, 4.8164, 4.7501, 4.8795, 4.8135,\n",
      "        4.8135, 4.8135, 4.8135, 4.8135, 4.8135, 4.8135, 4.8135, 4.8135, 4.8135,\n",
      "        4.8135, 4.8135, 4.8135, 4.8135, 4.8135, 4.8135, 4.8135, 4.8135, 4.8135,\n",
      "        4.8135, 4.8135, 4.8135, 4.8135, 4.8135, 4.8135, 4.8135, 4.8135, 4.8135,\n",
      "        4.8135, 4.8135, 4.8135, 4.8135, 4.9419, 4.8763, 5.0037, 4.9385, 4.8737,\n",
      "        4.8095]), 'prediction': True, 'confidence': 0.9999992435486227}\n"
     ]
    }
   ],
   "source": [
    "paraphrased_score_dict = watermark_detector.detect(paraphrased) # or any other text of interest to analyze\n",
    "\n",
    "print(paraphrased_score_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd7223c",
   "metadata": {},
   "source": [
    "# T=t\n",
    "\n",
    "Generate output text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddd20e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c72b45c8",
   "metadata": {},
   "source": [
    "Evaluate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2915c701",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cc6b4039",
   "metadata": {},
   "source": [
    "Simultate paraphrased text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decbc7a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "80185c30",
   "metadata": {},
   "source": [
    "Evaluate paraphrased text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f477cd8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
